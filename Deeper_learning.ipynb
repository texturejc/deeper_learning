{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb2d80e-bcb7-4589-8af6-3be9b42deb02",
   "metadata": {},
   "source": [
    "# Very little, almost nothing, on the nature of deep learning\n",
    "\n",
    "<img src=\"deeper_learning.png\" alt=\"Nested\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c29b6f-2000-4cef-8995-7e94f291c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!python -m spacy download en_core_web_md\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.keras import layers, models\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8aa22e3-00bd-4fbc-b7f4-35d4a2d0923b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h1 style=\"text-align: center;\">I have seen things...</h1>\n",
       "<video width=\"1400\" height=\"900\" controls loop>\n",
       "  <source src=\"fading.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_url = \"fading.mp4\"\n",
    "\n",
    "html_code = f\"\"\"\n",
    "<h1 style=\"text-align: center;\">I have seen things...</h1>\n",
    "<video width=\"1400\" height=\"900\" controls loop>\n",
    "  <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecf5db-5218-4857-95c0-77f61a7e22c8",
   "metadata": {},
   "source": [
    "## The TL;DR of deep learning\n",
    "\n",
    ">\\[Deep learnng] is a type of machine learning, a technique that enables computer systems to improve with experience and data. \\[...] \\[M]achine learning is the only viable approach to building AI systems that can operate in complicated real-world environments. Deeep learning is a particular kind of machine learning that achieves great power and flexibility by representing the world as a nested hierarchy of concepts, with each concept defined in terms of simpler concepts, and more abstract representations computed in terms of less abstract ones. (8)\n",
    ">\n",
    "> –– <cite>Goodfellow at al. (2016)</cite>\n",
    "\n",
    "<img src=\"DL.png\" alt=\"Nested\" width=\"700\"/>\n",
    "\n",
    "Goodfellow, I., Bengio, Y, and Courville, A. (2016). <i>Deep Learning</i>. Cambridge MA: The MIT Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac8358-e75a-4558-a19b-6707e0f3f4de",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffff; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "## 1. The basic unit of learning: The artificial neuron\n",
    "\n",
    "Let's create  the most basic possible neural network: A switch that turns on a light when the environment gets sufficiently dark. The elements of this system are as follows:\n",
    "\n",
    "* <b>An input signal, $x$</b>: The light guage\n",
    "* <b>A weight, $w$</b>: The negative value of the light signal (the brighter the light, the smaller the weight)\n",
    "* <b>A bias, $b$</b>: The propensity of the neuron to fire\n",
    "* <b>A summation function, $z$</b>: The combination of the input signal, the weight, and the bias\n",
    "* <b>An activation function, $f(z)$</b>: Determines the level at which the neuron fires or not\n",
    "\n",
    "![neuron](neuron.png)\n",
    "\n",
    "### Sunshine\n",
    "<img src=\"sun.png\" alt=\"Nested\" width=\"200\"/>\n",
    "\n",
    "\n",
    "* $x = 100$ lumens\n",
    "* $w = (-1)$\n",
    "* $b = 50$ units\n",
    "* $z = -1 \\cdot 100+50 = -50$ units\n",
    "* $f(z) = \\text{(Keep light OFF (0))}$\n",
    "\n",
    "### Moonshine\n",
    "\n",
    "<img src=\"moon.png\" alt=\"Nested\" width=\"200\"/>\n",
    "\n",
    "* $x = 5$ lumens\n",
    "* $w = (-1)$\n",
    "* $b = 50$ units\n",
    "* $z = -1 \\cdot 5+50 = 45$ units\n",
    "* $f(z) = \\text{(Turn light ON (I))}$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcca763-1881-41fe-b17b-8512e1cbffff",
   "metadata": {},
   "source": [
    "## 2. Networks of neurons\n",
    "\n",
    "In the previous example, it is easy to establish a rule for when the light should be switched on. Once we know the threshold (here, 0), we can adjust the weight and/or the bias to ensure that the light goes on or off when the threshold is crossed. But in many siutuations we cannot do this: at best, we have a fuzzy heuristic that guides us rather than a precise rule that we can easily communicate. How do we solve this problem?\n",
    "\n",
    "> <b>Deep learning</b> resolves this issues by <i>learning from examples</i>. That is, it takes a neural network with random weights and random biases, and adjusts these weights and biases until the network generates the same outputs as the training examples. For this, a single neuron is not enough––on grounds of both efficiency and its [inability to learn complex, non-linear functions like $XOR$ (exclusive $OR$)](https://automaticaddison.com/linear-separability-and-the-xor-problem/). \n",
    "\n",
    "Instead, neurons are aggregated into networks that consist of multiple inputs and least two––though usually more––neurons in the hidden layer. The outputs of the hidden layer are passed to the final layer, which uses an activation function to pick one of the outputs. In the network below, there are three inputs and five neurons in the hidden layer. The activation function––here, the sigmoid function––takes the states of the neurons in the hidden layer and maps it into a probability. Consider the network below, which could be used for a binary classification task:\n",
    "\n",
    "![five_neuron](nn.png)\n",
    "\n",
    "This is what's known as a <i>feedforward</i> network, as information passes through it in one direction only: from input to output––there is no feedback mechanism. Though it seems complicated, it is strinctly analagous with the single neuron example––the only difference is that the second network amalgamates the weights and biases to produce the output from three inputs. This happens in the following steps:\n",
    "\n",
    "1. Three input signals are given by $x_1$, $x_2$, and $x_3$.\n",
    "2. Each of the three input signals is sent to every neuron in the hidden layer. Each neuron multiplies the input signal by the relevant connection weight and adds the bias ($w_{i}x_{j}+b_n$); for a neuron $n$ in the hidden layer, the resuts are summed across all three inputs to produce $z_n$: $z_n = \\sum_{i=1}^{3}{w_{in}x_i+b_n}$.\n",
    "4. An activation function (here, the sigmoid function) is applied to each $z_n$ producing an activation of $a_n$: $a_n = \\frac{1}{1 + e^{-z_n}}$.\n",
    "5. The activation value for each neuron in the hidden layer is sent to the single neuron on the output layer. This neuron adds up the product of the connecting weights and the activation values for each preceding neuron and adds a bias, $b_{output}$. This gives: $z_{output}= w_{21} a_1 + w_{22} a_2 + w_{23} a_3 + w_{24} a_4 + w_{25} a_5 + b_{output}$.\n",
    "6. The last step comes with applying the activation function to $z_{output}$ to give $y$: $y = \\frac{1}{1 + e^{-z_{output}}}$\n",
    "\n",
    "What does this look like practice? Let's work through an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f77d89-5ed8-4a49-9a3a-cbabe36a74f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random values for all parameters\n",
    "inputs = np.random.uniform(-100., 100., [3])\n",
    "hidden_bias = np.random.rand(5)\n",
    "output_bias = np.random.rand(1)\n",
    "hidden_weights = np.random.uniform(-1., 1., [15])\n",
    "output_weights = np.random.uniform(-1., 1., [5])\n",
    "\n",
    "# Define the activation function\n",
    "def activation(value):\n",
    "    a = 1/(1+np.exp(-value))\n",
    "    return a\n",
    "\n",
    "# Multiply weights by input signal\n",
    "hidden_layer_signal = np.array([inputs[0] * hidden_weights[:5], inputs[1] * hidden_weights[5:10], inputs[2] * hidden_weights[10:]])\n",
    "print(\"The inputs are:\\n\\n\", inputs, \"\\n\")\n",
    "print(\"The weights of the hidden layer are:\\n\\n\", hidden_weights, \"\\n\")\n",
    "print(\"The weights x inputs to the hidden layer are:\\n\\n\", hidden_layer_signal, \"\\n\")\n",
    "print(\"The biases of hidden layer are:\\n\\n\", hidden_bias, \"\\n\")\n",
    "\n",
    "# Sum across the inputs for each neuron and add the bias\n",
    "hidden_result = hidden_layer_signal.sum(axis = 0) + hidden_bias\n",
    "print(\"Adding the three inputs for each neuron in the hidden layer to the bias for that neuron gives z_n:\\n\\n\", hidden_result, \"\\n\")\n",
    "\n",
    "# Apply the activation function the output from the hidden layer\n",
    "active_hidden = activation(hidden_result)\n",
    "print(\"Applying the activation function to each z_n gives a_n:\\n\\n\", active_hidden, \"\\n\")\n",
    "\n",
    "# Muliply the activations from the hidden later by the output layer weights:\n",
    "output_signal = output_weights * active_hidden \n",
    "print(\"The bias of the output layer neuron, b_output, is:\\n\\n\", output_bias, \"\\n\")\n",
    "\n",
    "# Sum the signals receieved the output layer and add the bias:\n",
    "z_output = np.sum(output_signal) + output_bias\n",
    "print(\"Summing the signals and adding the bias for the outoput layer gives:\\n\\n\", z_output, \"\\n\")\n",
    "\n",
    "# Apply the activation function to z_ouput:\n",
    "output = activation(z_output)\n",
    "print(\"Applying the activation function to z_output gives the network output, which is:\\n\\n\", output, \"\\n\")\n",
    "\n",
    "if output >0.5:\n",
    "    print(\"The predicted class is 1\")\n",
    "else:\n",
    "    print(\"The predicted class is 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60cd66-f5b1-4280-8b17-b162615be933",
   "metadata": {},
   "source": [
    "## 3. Loss functions\n",
    "\n",
    "> #### With four parameters I can fit an elephant, and with five I can make him wiggle his trunk\n",
    ">––John von Neumann\n",
    "\n",
    "\n",
    "Our network has 26 paramaters: 20 weights and six biases. The task of deep learning is to estimate values for these paramaters that allow the network to accurately predict the correct classification for a specific input. This is usually done by training the model on a sample of data where already have many examples of a correect classification $(x_1, x_2, x_3) \\rightarrow y$. This consists of the following steps:\n",
    "\n",
    "1. Initialise the network with random values for its parameters\n",
    "2. Present the network with inputs for which there is a known output value\n",
    "3. Measure the gap between the predicted value and the known value––this is the <i>loss</i> of the model.\n",
    "4. Establish the difference a small change in each parameter makes to the model's loss––does it make it bigger or smaller? (This is the <i>gradient</i> of the loss function.)\n",
    "5. Average the loss over lots of training examples.\n",
    "6. Adjust the parameters values in the direction of smaller loss.\n",
    "7. When the loss stops falling (or gets worse) stop training.\n",
    "\n",
    "The first issue we need to tackle here is the notion of a <b>loss function</b>. This is function that measures the error of the model. There are many examples of loss functions, but the most common is the <i>Mean Squared Error</i> (MSE):\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "Here, $y_1$ represents the true value of the input $x_i$, and $\\hat y_i$ the model's predicted value. This is then averaged across all inputs (note that $x_i$ may denote a set of inputs rather than a single value). Intuition for the MSE can be found readily enougn from a linear regression. For every value of $x$, the blue dot represents the true value and the black line the predicted value. The red lines (known as residuals) are the difference between the two. These are squared and averaged to get the MSE.\n",
    "\n",
    "![MSE](MSE.png)\n",
    "\n",
    "Let's imagine that a given input, $x_i$, has a true output of $1$. What happens as the estimated value varies? This gives the following result for the squared error (not the MSE!):\n",
    "\n",
    "1. $SE = (1-\\hat y)^2$\n",
    "2. $SE = (1-\\hat y)(1-\\hat y)$\n",
    "3. $SE = 1-\\hat y -\\hat y + \\hat y^2$\n",
    "4. $SE = \\hat y^2 - 2\\hat y +1$\n",
    "\n",
    "Plotting this gives the following curve for the value of the $SE$ as a function of $\\hat y$:\n",
    "\n",
    "![Squared Error](SE_min_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc209ee5-de74-46fc-952a-272d28881268",
   "metadata": {},
   "source": [
    "## The rate of change of the loss function\n",
    "\n",
    "How do we minimise the error in our lost function? mathematically, we do this by finding where the rate of the change of function is zero. The rate of change is defined as the slope of the tangent. We can, for instance, get measure the rate of change of the $SE$ by taking slope of the tangent at $\\hat{y_i} = 3$.\n",
    "\n",
    "![tangent](SE_tangent_plot.png)\n",
    "\n",
    "In practice, we get the rate of change of the loss function by using calculus. This involves differentiating the function, which gives us an expressing for the slope of a tangent at any point on a curve. Differentiation is a huge topic that we don't have time to go into, but it is straightforward to differentiate our $SE$ function and get the rate of change of the $SE$ with respect to $\\hat{y_i} = 3$:\n",
    "\n",
    "$$\\frac{dSE}{d\\hat{y_i}} = 2\\hat{y_i} -2$$\n",
    "\n",
    "Here, the $d$ symbol should be read as 'delta', with $dSE$ meaning 'a small quantity of $SE$'. The ratio $\\frac{dSE}{d\\hat{y_i}}$ is called the derivative, and by plugging in values for $\\hat{y_i}$ we can get the slope of the tangent at that point. What makes this especially useful is that it allows us to figure out the minimum points in our loss function. Specifically, by letting the derivative equal to zero and solving for $\\hat{y_i}$, we get the value of $\\hat{y_i}$ for which the error is zero:\n",
    "\n",
    "1. $2\\hat{y_i} -2 = 0$\n",
    "2. $2\\hat{y_i} = 2$\n",
    "3. $\\hat{y_i} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6752e-68e4-4b64-ba90-93972f17b69c",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Backpropagation is how the network updates its parameters. It does this by taking a mathematical expression for the loss of every parameter on in the network and changing the value of each parameter in ther direction of the smallest loss. In our neural network, like most neural networks, we have a series of functions, where the output of one function is the input into another. For example, the hidden layer activations feed into the output layer. This means that any attempt to minimise the loss function must ultimately operate on all the parameters of the model. In our example, the model outputs a value $y$ between $0$ and $1$. This value is computed by plugging the value of $z_{output}$ into the sigmoid function. This means that the Loss function, $L(y)$, also depends on $z_{output}$. However, $z_{output}$ in its turn depends on the activation values of the outputs and activations of the the hidden layer: $z_{output} = w_{21} z_1 + w_{22} z_2 + w_{23} z_3 + w_{24} z_4 + w_{25} z_5 + b_{output}$. In their turn, the weights and biases of the hidden layer depend on the weights and inputs of the input layer. How do calculate the rate of change of the loss function in this situation? The <b>chain rule of calculus</b> enables us to to do this. This says that for any functions $z = (y)$ and $y = (x)$, then:\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy}\\cdot\\frac{dy}{dx}$$\n",
    "\n",
    "What this allows us to do is to express the loss function in terms of the network inputs, and calculate the individual contributions each weight (via the activation function) makes to this loss. We're not going to derive all of thse (though it's relatively straighforward); instead we'll trace the impact of a single input through the network. (Remember that the $z_n$ variable here is a function of the network weights, so it 'contains' them.)\n",
    "\n",
    "$$ \\text{Weights: }\\frac{dy}{dx_i} = \\frac{dy}{dz_{\\text{output}}} \\cdot \\frac{dz_{\\text{output}}}{da_j} \\cdot \\frac{da_j}{dz_j} \\cdot \\frac{dz_j}{dx_i}$$\n",
    "\n",
    "As the biases also impact on the loss, we have a similar expression for the bias in the hidden layer:\n",
    "\n",
    "$$\n",
    "\\text{Biases: }\\frac{d y}{d b_j} = \\frac{d y}{d z_{\\text{output}}} \\cdot \\frac{d z_{\\text{output}}}{d a_j} \\cdot \\frac{d a_j}{d z_j} \\cdot \\frac{d z_j}{d b_j}\n",
    "$$\n",
    "\n",
    "Backpropagation works by randomly assigning a value to each parameter, calculating the loss, then adjusting each paramater in the direction of lower loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d366d-7941-477e-a3c0-ccedc04f5cd3",
   "metadata": {},
   "source": [
    "### Single neuron example\n",
    "\n",
    "Let's return our single neural network and take an example: the weight $w_{21}$. Let's assume our loss function is the $MSE$, the error associated with a fixed input. Out job is the figure out the loss associated with this specific weight and adjust the weight in the direction of the lower loss. This would be done for every weight and bias in the network.\n",
    "\n",
    "![five_neuron](nn.png)\n",
    "\n",
    "#### 1. Mean Squared Error (MSE) Loss Function:\n",
    "The loss for a single input is defined as:\n",
    "\n",
    "$L = \\frac{1}{2} (y_{\\text{pred}} - y_{\\text{true}})^2$\n",
    "\n",
    "where:\n",
    "\n",
    "* $y_{\\text{pred}}$ is the predicted output of the network.\n",
    "* $y_{\\text{true}}$ is the true label.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Network Output and Activation:\n",
    "The output of the network is given by:\n",
    "\n",
    "$y = \\sigma(z_{\\text{output}}) = \\frac{1}{1 + e^{-z_{\\text{output}}}}$\n",
    "\n",
    "where:\n",
    "\n",
    "$z_{\\text{output}} = w_{21} a_1 + w_{22} a_2 + w_{23} a_3 + w_{24} a_4 + w_{25} a_5 + b_{\\text{output}}$\n",
    "\n",
    "and $ a_1, a_2, a_3, a_4, a_5$) are the activations of the hidden layer neurons:\n",
    "\n",
    "$a_j = \\sigma(z_j) = \\frac{1}{1 + e^{-z_j}}$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Gradient of Loss with Respect to Network Output:\n",
    "First, we differentiate the loss function with respect to $y$. (Note that the $\\partial$ symbol here means we're taking a partial derivative: we're only interested in the effects of varying $w_{21}$ and the functions it feeds into, and are keeping every other weight and bias fixed.)\n",
    "\n",
    "$\\frac{\\partial L}{\\partial y} = (y_{\\text{pred}} - y_{\\text{true}})$\n",
    "\n",
    "Since the network output uses the sigmoid activation function, we compute:\n",
    "\n",
    "$\\frac{\\partial y}{\\partial z_{\\text{output}}} = y_{\\text{pred}} (1 - y_{\\text{pred}})$\n",
    "\n",
    "Thus, by the chain rule:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial z_{\\text{output}}} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot y_{\\text{pred}}(1 - y_{\\text{pred}})$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Gradient of Loss with Respect to $w_{21}$:\n",
    "Since $z_{\\text{output}}$ depends on $w_{21}$, we compute:\n",
    "\n",
    "$\\frac{\\partial z_{\\text{output}}}{\\partial w_{21}} = a_1$\n",
    "\n",
    "Applying the chain rule:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_{21}} = \\frac{\\partial L}{\\partial z_{\\text{output}}} \\cdot \\frac{\\partial z_{\\text{output}}}{\\partial w_{21}}$\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_{21}} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot y_{\\text{pred}}(1 - y_{\\text{pred}}) \\cdot a_1$\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: Final Gradient Expression:\n",
    "$\\frac{\\partial L}{\\partial w_{21}} = (y_{\\text{pred}} - y_{\\text{true}}) \\cdot y_{\\text{pred}}(1 - y_{\\text{pred}}) \\cdot a_1$\n",
    "\n",
    "This expression gives the gradient of the loss function with respect to the weight $w_{21}$, which is used to update the weight during gradient descent. \n",
    "\n",
    "* If  $\\frac{\\partial L}{\\partial w_{21}} > 0$, then we decrease $w_{21}$ (move in the negative gradient direction)\n",
    "* If $\\frac{\\partial L}{\\partial w_{21}} < 0$, then we increase $w_{21}$ (move in the positive gradient direction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c8affa-b8d9-40b2-8d63-81cdca06bf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h1 style=\"text-align: center;\">Because you've never seen a miracle...</h1>\n",
       "<video width=\"1400\" height=\"900\" controls>\n",
       "  <source src=\"blade_runner.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_url = \"blade_runner.mp4\"\n",
    "\n",
    "html_code = f\"\"\"\n",
    "<h1 style=\"text-align: center;\">Because you've never seen a miracle...</h1>\n",
    "<video width=\"1400\" height=\"900\" controls>\n",
    "  <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4197871c-29fd-4e2a-b2e3-156cb1d5c8c1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f6f6f6; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "## The miracle of the loaves and fishes\n",
    "\n",
    "![loaves](loaves.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125fe9a-932a-4abf-bc72-62309984a58d",
   "metadata": {},
   "source": [
    "### Let's first train a network on some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0fd37-33f8-4ab1-ae4d-905a99f9068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = models.Sequential([\n",
    "    layers.Dense(5, activation='sigmoid', input_shape=(3,)),  # Hidden layer with 5 neurons\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer (binary classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Generate dummy data (100 samples, 3 input features)\n",
    "X = np.random.rand(100, 3)  # 100 samples, 3 features\n",
    "y = np.random.randint(0, 2, size=(100, 1))  # 100 binary labels (0 or 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = model.predict(X_test)  # Get probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "# Generate and print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Retrieve weights and biases\n",
    "for layer in model.layers:\n",
    "    weights, biases = layer.get_weights()\n",
    "    print(f\"\\nLayer: {layer.name}\")\n",
    "    print(f\"Weights:\\n{weights}\")\n",
    "    print(f\"Biases:\\n{biases}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0dbfc-9f45-4fe9-bf70-6cca95e196bc",
   "metadata": {},
   "source": [
    "## Now let's train a network on some language data––in this case, food words and non-food words\n",
    "\n",
    "We can do this by taking the word embeddings for food words from the `spaCy` medium language model. Word embeddings are 300-dimensional vectors of weights that can be used to predict the co-occurrence of words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde1509-ecc2-4460-b42b-e18aa4ed78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the medium language model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "words = [\n",
    "        # 🍕 Food words (1)\n",
    "        \"apple\", \"banana\", \"carrot\", \"bread\", \"cheese\", \"chicken\", \"chocolate\", \"coffee\", \"cookie\", \"donut\",\n",
    "        \"egg\", \"fish\", \"grape\", \"honey\", \"icecream\", \"jam\", \"ketchup\", \"lemon\", \"mango\", \"milk\",\n",
    "        \"noodles\", \"orange\", \"pancake\", \"pepper\", \"pizza\", \"popcorn\", \"pumpkin\", \"rice\", \"salad\", \"salt\",\n",
    "        \"sandwich\", \"sausage\", \"soup\", \"spaghetti\", \"spinach\", \"strawberry\", \"sugar\", \"sushi\", \"tea\", \"tomato\",\n",
    "        \"turkey\", \"vanilla\", \"waffle\", \"watermelon\", \"yogurt\", \"zucchini\", \"beef\", \"pasta\", \"coconut\", \"burger\",\n",
    "        \n",
    "        # 🚫 Non-food words (0)\n",
    "        \"car\", \"bottle\", \"chair\", \"laptop\", \"phone\", \"television\", \"pencil\", \"candle\", \"mirror\", \"window\",\n",
    "        \"book\", \"notebook\", \"desk\", \"computer\", \"camera\", \"keyboard\", \"mouse\", \"lamp\", \"sofa\", \"door\",\n",
    "        \"shoes\", \"sock\", \"jacket\", \"shirt\", \"trousers\", \"hat\", \"watch\", \"wallet\", \"backpack\", \"glasses\",\n",
    "        \"earphones\", \"radio\", \"guitar\", \"piano\", \"violin\", \"painting\", \"clock\", \"newspaper\", \"magazine\", \"bicycle\",\n",
    "        \"bus\", \"train\", \"plane\", \"ship\", \"road\", \"bridge\", \"building\", \"island\", \"mountain\", \"river\"\n",
    "    ]\n",
    "\n",
    "classes = [\n",
    "        # 🍕 1 for Food Words\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "\n",
    "        # 🚫 0 for Non-Food Words\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    ]\n",
    "\n",
    "vectors = [nlp(i).vector for i in words]\n",
    "\n",
    "vecs_df = pd.DataFrame(vectors)\n",
    "vecs_df['word'] = words\n",
    "vecs_df['label'] = classes\n",
    "vecs_df = vecs_df.sample(frac = 1)\n",
    "\n",
    "X = vecs_df.drop(['word', 'label'], axis = 1)\n",
    "y = vecs_df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8b2ad-21e3-4a2e-9277-a1ec08c0f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = models.Sequential([\n",
    "    layers.Dense(5, activation='relu', input_shape=(300,)),  # Hidden layer with 5 neurons\n",
    "    layers.Dense(1, activation='sigmoid')  # Output layer (binary classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=5, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = model.predict(X_test)  # Get probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "# Generate and print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Retrieve weights and biases\n",
    "for layer in model.layers:\n",
    "    weights, biases = layer.get_weights()\n",
    "    print(f\"\\nLayer: {layer.name}\")\n",
    "    print(f\"Weights:\\n{weights}\")\n",
    "    print(f\"Biases:\\n{biases}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee80dd-5497-4e97-92f4-6270ce88a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(term):\n",
    "    word = nlp(term).vector.reshape(1,-1)\n",
    "    p = model.predict(word)\n",
    "    if p[0][0] >=0.5:\n",
    "        return {'food': float(p[0][0])}\n",
    "    else:\n",
    "        return {'not food': float(p[0][0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a15d93-a814-43a0-b195-7ee98203e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred('salad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9fa194-abac-4241-ac5b-bbb9a96fb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemon = np.array(nlp('lemon').vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c66ea-e0d0-4e56-be01-602f11583a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    weights, biases = layer.get_weights()\n",
    "    print(f\"\\nLayer: {layer.name}\")\n",
    "    print(f\"Weights:\\n{weights}\")\n",
    "    print(f\"Biases:\\n{biases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f2d46b-cd1f-4c7d-920d-bc4aa9a5b205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<h1 style=\"text-align: center;\">All the best memories are hers...</h1>\n",
       "<video width=\"1400\" height=\"900\" controls>\n",
       "  <source src=\"memories_.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_url = \"memories_.mp4\"\n",
    "\n",
    "html_code = f\"\"\"\n",
    "<h1 style=\"text-align: center;\">All the best memories are hers...</h1>\n",
    "<video width=\"1400\" height=\"900\" controls>\n",
    "  <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2967557e-21ec-4a19-aad8-6f4d7f3e7c61",
   "metadata": {},
   "source": [
    "# What is thinking?\n",
    "\n",
    "<img src=\"mic_.png\" alt=\"Nested\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030d217-841a-43bc-b896-06e311adfbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
